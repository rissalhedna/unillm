{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and storage\n",
    "The following sections will handle the scraped data stored in json files. We start by opening our data file, in our case it's raw_study_in_germany_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the crawler's data from the output file\n",
    "\n",
    "import json\n",
    "with open('../data/raw_study_in_germany_data.json', 'r', encoding='utf-8') as file:\n",
    "    raw_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create functions to handle and remove personal information from the raw scraped data. We also make an LLM call (OpanAI API) for further processing and to define whether the resulting information is relevant to students who would like to study/live in Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import spacy\n",
    "\n",
    "# Optional: Import OpenAI if using LLM-based cleaning\n",
    "try:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # Ensure your API key is set as an environment variable\n",
    "    client = OpenAI()\n",
    "except ImportError:\n",
    "    OpenAI = None\n",
    "    print(\"OpenAI library not found. LLM-based cleaning will be skipped.\")\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\"Remove email addresses from text.\"\"\"\n",
    "    email_pattern = r'\\S+@\\S+'\n",
    "    return re.sub(email_pattern, '[EMAIL]', text)\n",
    "\n",
    "def remove_phone_numbers(text):\n",
    "    \"\"\"Remove phone numbers from text.\"\"\"\n",
    "    phone_pattern = r'\\+?\\d[\\d -]{8,}\\d'\n",
    "    return re.sub(phone_pattern, '[PHONE]', text)\n",
    "\n",
    "def remove_names(text):\n",
    "    \"\"\"Remove person names using spaCy's NER.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "    cleaned_text = text\n",
    "    # Replace names with [NAME]\n",
    "    for start, end, label in reversed(entities):\n",
    "        cleaned_text = cleaned_text[:start] + '[NAME]' + cleaned_text[end:]\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_addresses(text):\n",
    "    \"\"\"Remove addresses using regex and spaCy.\"\"\"\n",
    "    # Simple regex pattern for addresses (can be enhanced)\n",
    "    address_pattern = r'\\d{1,5}\\s\\w+\\s(?:Street|St|Avenue|Ave|Boulevard|Blvd|Road|Rd|Lane|Ln|Drive|Dr)\\b[\\w\\s,.-]*'\n",
    "    text = re.sub(address_pattern, '[ADDRESS]', text)\n",
    "    \n",
    "    # Using spaCy to remove GPE (Geopolitical Entities) as a proxy for locations\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    cleaned_text = text\n",
    "    # Replace locations with [LOCATION]\n",
    "    for start, end, label in reversed(entities):\n",
    "        cleaned_text = cleaned_text[:start] + '[LOCATION]' + cleaned_text[end:]\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_personal_info(text):\n",
    "    \"\"\"Aggregate function to remove various personal information.\"\"\"\n",
    "    text = remove_emails(text)\n",
    "    text = remove_phone_numbers(text)\n",
    "    text = remove_names(text)\n",
    "    text = remove_addresses(text)\n",
    "    return text\n",
    "\n",
    "def define_relevance_with_llm(text):\n",
    "    relevance_prompt=f\"\"\"\n",
    "    You are an automated classifier designed to evaluate whether a given text \n",
    "    contains relevant information for students interested in studying and living\n",
    "    in Germany. Your task is to read the provided text and determine its relevancy\n",
    "    based on the following criteria:\n",
    "\n",
    "    Respond with \"Yes\" if the text includes information such as:\n",
    "\n",
    "    Study opportunities (e.g., universities, courses, scholarships)\n",
    "    Living conditions (e.g., cost of living, accommodation tips)\n",
    "    Cultural insights (e.g., cultural norms, language tips)\n",
    "    Practical advice for adapting to life in Germany\n",
    "    Local resources and support systems for international students\n",
    "    Respond with \"No\" if the text:\n",
    "\n",
    "    Does not pertain to studying or living in Germany\n",
    "    Contains irrelevant or unrelated information\n",
    "    Focuses on personal anecdotes without broader applicability\n",
    "    Discusses topics outside the scope of student life in Germany\n",
    "    Examples:\n",
    "\n",
    "    Text: \"Discover the top universities in Germany offering scholarships\n",
    "    for international students. Learn about the application process and tips\n",
    "    for living on a student budget.\"\n",
    "\n",
    "    Response: Yes\n",
    "\n",
    "    Text: \"The weather today is sunny with a high of 25Â°C. It's a perfect day\n",
    "    for a picnic in the park.\"\n",
    "\n",
    "    Response: No\n",
    "\n",
    "    Text: \"Navigating the German public transportation system can be challenging\n",
    "    for newcomers. Here's a guide to help you get around efficiently.\"\n",
    "\n",
    "    Response: Yes\n",
    "\n",
    "    Text: \"I went hiking in the Alps last summer and it was an unforgettable experience.\"\n",
    "\n",
    "    Response: No\n",
    "\n",
    "    Now, evaluate the following text:\n",
    "\n",
    "    Text: \"{text}\"\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": relevance_prompt},\n",
    "        ])\n",
    "        \n",
    "        relevance = response.choices[0].message.content\n",
    "        return relevance\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM-based relevance detection: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def clean_text_with_llm(text):\n",
    "    \"\"\"\n",
    "    Clean text using an LLM like OpenAI's GPT.\n",
    "    This function sends a prompt to the LLM to clean the text.\n",
    "    Ensure you have set the OPENAI_API_KEY environment variable.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        print(\"OpenAI library not available. Skipping LLM-based cleaning.\")\n",
    "        return text\n",
    "    \n",
    "    cleaning_prompt = f\"\"\"\n",
    "    You are an expert data cleaner tasked with processing scraped website data \n",
    "    to create clear and informative content for students interested in studying\n",
    "    and living in Germany. The input text may contain disorganized information and \n",
    "    personal details. Your objectives are to:\n",
    "\n",
    "    Extract Relevant Information:\n",
    "\n",
    "    Identify and retain only the information pertinent to studying and living in \n",
    "    Germany, such as educational opportunities, living conditions, cultural insights,\n",
    "    accommodation tips, and local resources.\n",
    "    \n",
    "    Remove Personal Information:\n",
    "\n",
    "    Exclude all personal data, including names, contact details, addresses, email addresses,\n",
    "    phone numbers, and any other identifying information.\n",
    "    Ensure Coherence and Clarity:\n",
    "\n",
    "    Organize the extracted information into a well-structured, coherent, and readable format.\n",
    "    Maintain the original meaning and intent of the content without adding or omitting critical information.\n",
    "    \n",
    "    Original Text: \"{text}\"\n",
    "\n",
    "    Cleaned Text:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": cleaning_prompt},\n",
    "        ])\n",
    "        \n",
    "        cleaned_text = response.choices[0].message.content\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM-based cleaning: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all our previous functions to clean the data. Then store it in a separate json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(INPUT_FILE = '../data/test_data_small.json',\n",
    "CLEANED_FILE = '../data/cleaned_study_in_germany_data.json'):\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file '{INPUT_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            print(f\"Loaded {len(data)} data points from '{INPUT_FILE}'.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"\\nInitial DataFrame Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    required_columns = ['url', 'title', 'text_content']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Missing required column: '{col}'. Please check your data.\")\n",
    "            return\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df.drop_duplicates(subset=['url'], inplace=True)\n",
    "    duplicates_removed = initial_count - len(df)\n",
    "    print(f\"\\nRemoved {duplicates_removed} duplicate entries based on 'url'.\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df.dropna(subset=['url', 'text_content'], inplace=True)\n",
    "    missing_removed = initial_count - len(df)\n",
    "    print(f\"Removed {missing_removed} entries with missing 'url' or 'text_content'.\")\n",
    "\n",
    "    titles_filled = df['title'].isnull().sum()\n",
    "    df['title'].fillna('No Title', inplace=True)\n",
    "    print(f\"Filled {titles_filled} missing 'title' entries with 'No Title'.\")\n",
    "\n",
    "    print(\"\\nRemoving personal information...\")\n",
    "    df['text_content'] = df['text_content'].apply(remove_personal_info)\n",
    "\n",
    "    apply_llm = True \n",
    "    if apply_llm:\n",
    "        print(\"Applying LLM-based cleaning...\")\n",
    "        df['text_content'] = df['text_content'].apply(clean_text_with_llm)\n",
    "\n",
    "    df.to_json(CLEANED_FILE, orient='records', indent=4)\n",
    "    print(f\"\\nCleaned data saved to '{CLEANED_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a cleaned json file, we can now convert it to a txt file for storage in a vector database (Alternatively, we can pause here and return later since our data is present locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_study_in_germany_data.json', 'r', encoding='utf-8') as file:\n",
    "    clean_data = json.load(file)\n",
    "    \n",
    "output_txt_path = \"../data/clean_study_in_germany_data.txt\"\n",
    "\n",
    "if isinstance(clean_data, list):\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "        for entry in clean_data:\n",
    "            if isinstance(entry, dict) and 'text_content' in entry:\n",
    "                text = entry['text_content']\n",
    "                txt_file.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the data in a vector database. We are using QDRANT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.storage_utils import query_qdrant\n",
    "from app.utils.storage_utils import store_in_qdrant\n",
    "import qdrant_client\n",
    "import os\n",
    "\n",
    "# Get environment variables for Qdrant configuration\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "ENVIRONMENT = os.environ['ENVIRONMENT']\n",
    "\n",
    "# Initialize Qdrant client based on environment\n",
    "if ENVIRONMENT == 'dev':\n",
    "        # For local development, connect to localhost\n",
    "        client = qdrant_client.QdrantClient(\n",
    "        host=\"localhost\",\n",
    "        port=6333,)\n",
    "else:\n",
    "    # For production, connect to cloud instance using URL and API key\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY\n",
    "    )\n",
    "\n",
    "store_in_qdrant(client, \"study-in-germany\", directory_name=\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now query the vector database to test if it's working. The LLM will return the most relevant information based on the query. Any information that is not relevant to the query will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"On average how much do student working jobs pay in computer science ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"What rental platform should i use to find an apartment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"How can I \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
